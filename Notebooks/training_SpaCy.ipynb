{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install spacy -q"
      ],
      "metadata": {
        "id": "mhuaiHOqbs-Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy-transformers -q"
      ],
      "metadata": {
        "id": "a2CFiyNObvp1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WE4ayZYbMA6",
        "outputId": "d559e5f6-337c-4306-b504-1a70f3292819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "5CuDGDcB2z3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy train drive/MyDrive/DTA/Internship/spaCy/config.cfg --output drive/MyDrive/DTA/Internship/spaCy/output --paths.train drive/MyDrive/DTA/Internship/spaCy/training_data_TRAIN.spacy --paths.dev drive/MyDrive/DTA/Internship/spaCy/training_data_DEV.spacy --gpu-id 0"
      ],
      "metadata": {
        "id": "WYS5o_L6vYtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c412cd9-ee48-4e8d-99c4-0c099761bef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory:\n",
            "drive/MyDrive/DTA/Internship/spaCy/output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory:\n",
            "drive/MyDrive/DTA/Internship/spaCy/output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-01-19 17:36:33,176] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2023-01-19 17:36:33,187] [INFO] Pipeline: ['transformer', 'ner']\n",
            "INFO:spacy:Pipeline: ['transformer', 'ner']\n",
            "[2023-01-19 17:36:33,191] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2023-01-19 17:36:33,192] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "Downloading: 100% 254/254 [00:00<00:00, 301kB/s]\n",
            "Downloading: 100% 608/608 [00:00<00:00, 659kB/s]\n",
            "Downloading: 100% 242k/242k [00:00<00:00, 269kB/s]\n",
            "Downloading: 100% 112/112 [00:00<00:00, 117kB/s]\n",
            "Downloading: 100% 437M/437M [00:26<00:00, 16.3MB/s]\n",
            "Some weights of the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2023-01-19 17:37:37,824] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "INFO:spacy:Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        4833.91   1326.22    0.80    0.55    1.46    0.01\n",
            "  8     200      213417.79  90600.52   88.43   91.30   85.73    0.88\n",
            " 16     400        1574.14   6527.86   90.95   91.18   90.73    0.91\n",
            " 24     600         326.13   1133.62   91.73   92.13   91.34    0.92\n",
            " 32     800         158.15    589.37   91.76   90.51   93.05    0.92\n",
            " 40    1000         115.98    365.08   91.16   92.47   89.88    0.91\n",
            " 48    1200         120.45    347.38   92.39   92.96   91.83    0.92\n",
            " 56    1400         104.11    279.23   93.19   93.71   92.68    0.93\n",
            " 64    1600          71.13    207.01   92.70   93.22   92.20    0.93\n",
            " 72    1800          68.21    200.53   92.04   92.38   91.71    0.92\n",
            " 80    2000          69.63    187.83   92.35   93.50   91.22    0.92\n",
            " 88    2200          49.95    127.55   91.65   92.97   90.37    0.92\n",
            " 96    2400          45.84    129.17   91.64   92.43   90.85    0.92\n",
            "104    2600          42.65    120.97   91.00   92.67   89.39    0.91\n",
            "112    2800          58.46    150.47   92.13   92.19   92.07    0.92\n",
            "120    3000          51.26    132.94   92.27   93.60   90.98    0.92\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "drive/MyDrive/DTA/Internship/spaCy/output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " \\# column is the number of optimization steps (= batches processed)\n",
        " LOSS =  loss values for the transformer and named entity recognition steps in your pipeline\n",
        " ENTS_F = entities f score\n",
        " ENTS_P = entities precision\n",
        " ENTS_R = entities recall\n",
        " SCORE = overall score of the pipeline"
      ],
      "metadata": {
        "id": "Gm1hepQflVlr"
      }
    }
  ]
}